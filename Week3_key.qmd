---
title: "Week Three"
format: pdf
editor: source
---

## Last Week

- Inference for Proportions
  - Statistical probability distributions (Binomial & Multinomial)
  - MLE estimation for parameters
  - Estimation vs. Testing
  
- Bayesian Primer Video
  - Prior Distributions
  - Beta Distribution (for proportions)
  - Posteriors Distributions
  
## This Week: Bayesian Inference for Proportions

Today:

- Activity 2

    -   Bayesian Inference for Binomial & Multinomial Distributions
    -   Bayesian Inference vs. Maximum Likelihood Estimation

- Thursday: Lab 2

## Next Week: Contingency Tables

for Tuesday:

-   Watch Week 3 videos and submit HW 3 (video notes)

------------------------------------------------------------------------

#### Bayesian Inference Overview

The Bayesian statistics paradigm follows three basic steps.

1. Specify prior belief (distribution) about model parameters.

\vfill

2. Collect data (assumed to follow statistical probability distribution: Likelihood)

\vfill

3. Posterior distribution defined by Bayes rule: Prior + Likelihood $\rightarrow$ Posterior

\vfill

\newpage

#### Beta Distribution (Priors)

For modeling binary categorical data, we have seen how the binomial distribution is useful. The parameter we hope to estimate is $\pi$, which is restricted to be between 0 and 1.

\vfill

The beta distribution is a good distribution for this case.

$p(x) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha +\beta)} x^{\alpha-1} (1-x) ^{\beta-1}$, x $\in [0,1]$

\vfill

What are the parameters in this distribution (and what are the moments- mean & variance)?

_$\alpha$ and $\beta$_

_Mean = $\frac{\alpha}{\alpha + \beta}$_

_Var = $\frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}$_


How do they impact the shape of the distribution? Overlay curves with a wide range of parameter values.

```{r}
#| message: false
#| echo: false
library(tidyverse)
library(viridis)
num <- 100
x <- seq(0,1, length.out = num)

tibble(y = c(dbeta(x, 10, 1), 
             dbeta(x, 100, 10), 
             dbeta(x, 5, 1), 
             dbeta(x, 50, 10), 
             dbeta(x, 10, 10), 
             dbeta(x, 5, 5), 
             dbeta(x, 1, 1),
             dbeta(x, 1, 10), 
             dbeta(x, 1, 5)),
       x = rep(x, 9),
       parameters = c(rep('10,1', num),
                      rep('100,10', num),
                      rep('5,1', num),
                      rep('50,10', num),
                      rep('10,10', num), 
                      rep('5,5', num), 
                      rep('1,1', num),
                      rep('1,10', num), 
                      rep('1,5', num))) |>
  ggplot(aes(y=y, x=x, color = parameters)) +
  geom_line() +
  theme_minimal() +
  scale_color_viridis(discrete=TRUE) 
```

\newpage

#### Bayesian Estimation for Binary Data

On the first day of class, we intuitively estimated $\pi$ for the ice cream problem to be $\frac{7}{16}=$ `r round(7/16, 2)`, which was also the MLE.

\vfill

In this setting, the posterior distribution is exactly defined to be a Beta distribution with parameters $y + \alpha$ and $N - y + \beta$.

\vfill

Let's visualize this setting with a set of different prior distributions and add the posterior mean and MLE.

- Beta(.01, .01)

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
alpha <- .01
beta <- .01
pi_vals <- seq(0, 1, by = .001)
n <- length(pi_vals)
prior_vals <- dbeta(pi_vals, alpha,  beta)
posterior_vals <- dbeta(pi_vals, 7 + alpha, 9 + beta)
tibble(pi = rep(pi_vals,2), 
       vals = c(prior_vals, posterior_vals),
       type = rep(c('prior', 'posterior'), each = n))|>
  ggplot(aes(y = vals, x = pi, color = type)) + 
  geom_line() +
  theme_bw() +
  xlab(expression(pi)) +
  ggtitle("Posterior for pi with 7/16 for Sweet Peaks \n and Beta(.01,.01) prior") +
  geom_vline(xintercept = 7/16, linewidth = 2) +
  annotate('text', x = .3, y = .5, label = paste('MLE = ',round((7 )/(16),2 ))) +
  geom_vline(xintercept = (7 + alpha)/(16 + alpha + beta), color = 'purple', linetype = 2, linewidth = 2) +
  annotate('text', x = .55, y = .5, label = paste('MAP = ',round((7 + alpha)/(16 + alpha + beta),2 )), color = 'purple') +
  theme(legend.position = "bottom") + 
  ylab('')
```

\newpage

- Beta(1, 1)

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
alpha <- 1
beta <- 1
pi_vals <- seq(0, 1, by = .001)
n <- length(pi_vals)
prior_vals <- dbeta(pi_vals, alpha,  beta)
posterior_vals <- dbeta(pi_vals, 7 + alpha, 9 + beta)
tibble(pi = rep(pi_vals,2), 
       vals = c(prior_vals, posterior_vals),
       type = rep(c('prior', 'posterior'), each = n))|>
  ggplot(aes(y = vals, x = pi, color = type)) + 
  geom_line() +
  theme_bw() +
  xlab(expression(pi)) +
  ggtitle("Posterior for pi with 7/16 for Sweet Peaks \n and Beta(1,1) prior") +
  geom_vline(xintercept = 7/16, linewidth = 2) +
  annotate('text', x = .3, y = .5, label = paste('MLE = ',round((7 )/(16),2 ))) +
  geom_vline(xintercept = (7 + alpha)/(16 + alpha + beta), color = 'purple', linetype = 2, linewidth = 2) +
  annotate('text', x = .55, y = .5, label = paste('MAP = ',round((7 + alpha)/(16 + alpha + beta),2 )), color = 'purple') +
  theme(legend.position = "bottom") + 
  ylab('')
```

- Beta(7, 3)

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
alpha <- 7
beta <- 3
pi_vals <- seq(0, 1, by = .001)
n <- length(pi_vals)
prior_vals <- dbeta(pi_vals, alpha,  beta)
posterior_vals <- dbeta(pi_vals, 7 + alpha, 9 + beta)
tibble(pi = rep(pi_vals,2), 
       vals = c(prior_vals, posterior_vals),
       type = rep(c('prior', 'posterior'), each = n))|>
  ggplot(aes(y = vals, x = pi, color = type)) + 
  geom_line() +
  theme_bw() +
  xlab(expression(pi)) +
  ggtitle("Posterior for pi with 7/16 for Sweet Peaks \n and Beta(7,3) prior") +
  geom_vline(xintercept = 7/16, linewidth = 2) +
  annotate('text', x = .3, y = .5, label = paste('MLE = ',round((7 )/(16),2 ))) +
  geom_vline(xintercept = (7 + alpha)/(16 + alpha + beta), color = 'purple', linetype = 2, linewidth = 2) +
  annotate('text', x = .55, y = .5, label = paste('MAP = ',round((7 + alpha)/(16 + alpha + beta),2 )), color = 'purple') +
  theme(legend.position = "bottom") + 
  ylab('')
```


\newpage

#### Estimation: Bayesian Uncertainty intervals

Unlike a likelihood profile, with a Bayesian posterior distribution there is a natural way to obtain a 95% (or other level) uncertainty interval.

\vfill

Consider the posterior generated using a beta(.01, .01) prior, a Beta(7.01, 9.01) distribution. We can can simply trim the quantiles from the distribution. So the 95% interval boundaries are at `r round(qbeta(.025, 7.01, 9.01),2)` and `r round(qbeta(.975, 7.01, 9.01),2)`.

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
alpha <- .01
beta <- .01
pi_vals <- seq(0, 1, by = .001)
n <- length(pi_vals)
prior_vals <- dbeta(pi_vals, alpha,  beta)
posterior_vals <- dbeta(pi_vals, 7 + alpha, 9 + beta)
tibble(pi = rep(pi_vals,2), 
       vals = c(prior_vals, posterior_vals),
       type = rep(c('prior', 'posterior'), each = n))|>
  ggplot(aes(y = vals, x = pi, color = type)) + 
  geom_line() +
  theme_bw() +
  xlab(expression(pi)) +
  ggtitle("Posterior for pi with 7/16 for Sweet Peaks \n and Beta(.01,.01) prior") +
  theme(legend.position = "bottom") + 
  geom_vline(xintercept = qbeta(.025, 7.01, 9.01)) +
  geom_vline(xintercept = qbeta(.975, 7.01, 9.01)) +
  ylab('')
```

\vfill

Is this the only possible interval?

\vfill

\newpage

#### Multicategory Outcomes

Thus far, we have simplified the ice cream example to a binary situation (Sweet Peaks or not).

However, the data was collected as a multicategory setting with many possible answers.

An extension to the binomial distribution, known as the multinomial distribution is appropriate in this case.

$$P(Y_1= y_1, Y_2 = y_2, ..., Y_k = y_k) = \frac{n!}{y_1!y_2!...y_k!}\pi_1^{y_1}\pi_2^{y_2}...\pi_k^{y_k}  , \;\; \text{ where } y_i = 0, 1, 2, ...$$
\vfill

To estimate the parameters in this distribution we can use a Dirichlet distribution as a prior distribution, which is a multivariate extension to the beta distribution. 

$p(x) = \frac{\prod_i \Gamma(\alpha_i) }{\Gamma(\sum_i \alpha_i )} \prod_i \pi_i^{\alpha_i-1}$, \pi_i $\in [0,1]$

\vfill

Given the similarities to the previous example, what do you anticipate the form of the posterior distribution would look like in this case with multinomial data and a dirichlet prior.

\vfill

The posterior would be Dirichlet with parameters $\alpha_i + y_i$.

\newpage

Recall our data from the first day of class where we had the following data.

- Sweet Peaks 7
- Big Dipper 2
- Ben and Jerry's 2
- DQ 1
- Coldstone 1
- Wendys 1

So that we can visualize the distribution, lets consider just three options: Sweet Peaks, Big Dipper, and Ben and Jerry's.

Here is a prior distribution, given a Dirichlet prior where $\alpha_i = 1$.

```{r}
# Code modified from Google Gemini
library(ggtern)
library(MCMCpack)


dirichlet_samples <- as_tibble(rdirichlet(10000, c(1, 1, 1))) 
colnames(dirichlet_samples) <- c('SP', 'BD', 'BJ')


# Plot the Dirichlet distribution on a ternary plot
ggtern(data = dirichlet_samples, aes(x = SP, y = BD, z = BJ)) +
  geom_point(alpha = 0.25, size = 1) +
  labs(title = "Dirichlet Distribution for (1, 1, 1)",
       Tlab = "x1", Llab = "x2", Rlab = "x3") +
  theme_bw()
```

Modify this code to construct a posterior distribution and interpret the figure.


```{r}
post_samples <- as_tibble(rdirichlet(10000, c(1 + 7, 1 + 2, 1 + 2)))
colnames(post_samples) <- c('SP', 'BD', 'BJ')


# Plot the Dirichlet distribution on a ternary plot
ggtern(data = post_samples, aes(x = SP, y = BD, z = BJ)) +
  geom_point(alpha = 0.25, size = 1) +
  labs(title = "Dirichlet Distribution for (1, 1, 1)",
       Tlab = "x1", Llab = "x2", Rlab = "x3") +
  theme_bw()
```

From this distribution our mean estimates are:

- SP `r round((1 + 7)/ (1 + 7+ 1 + 2 + 1 + 2), 2)`
- BD `r round((1 + 2)/ (1 + 7+ 1 + 2 + 1 + 2), 2)`
- BJ `r round((1 + 2)/ (1 + 7+ 1 + 2 + 1 + 2), 2)`

We can also contruct uncertainty intervals

```{r}
rdirichlet(10000, c(8, 3, 3)) |> apply(2, quantile, probs = c(.025, .975))
```

