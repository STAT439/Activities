---
title: "Week Seven"
format: gfm
editor: source
editor_options: 
  chunk_output_type: console
---

## Last Week

- GLMs and Logistic Regression

- CODING! ( with the goal to understand model assumptions and specified functional relationship)


## This Week: More Generalized Linear Models

Video:

- Model fitting and intervals: IRLS + Wald (MLE) and MCMC (Bayes)

- Intrepreting model coefficients: curve fitting, probability, log odds / odds ratios.


Today:

- Activity:

    - Model checking and residuals
    - Model selection principles


- Thursday: Lab 

    - Separation

## Next Week: Midterm Exam Week

- Tuesday: Review Day + Take Home

- Thursday: In class (1 page handwritten study sheet permitted)


------------------------------------------------------------------------
```{r}
#| include: false
library(tidyverse)
library(knitr)
library(arm)
library(rstanarm)
library(readr)
set.seed(09262025)
```

\newpage

### Logistic Regression

Recall the logistic regression framework, which satisfies the three elements of a GLM (random component, systematic component, link function)

\begin{align*}
y & \sim Bernoulli(\pi) \\
\pi & = \frac{\exp(\beta_0 + \beta_1 x)}{1 + \exp(\beta_0 + \beta_1 x)}\\
\pi & = logit^{-1}(\beta_0 + \beta_1 x)\\
\end{align*}

We can also simultaneously consider several different predictor, both continuous and categorical. Additionally, we should also evaluate whether `y ~ x1 + x2` is appropriate or whether interactions or non-linear relationships are appropriate For example, 

$$\pi  = logit^{-1}(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2)$$
or
$$\pi  = logit^{-1}(\beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \beta_3 I(x_2 = A) +  \beta_3 I(x_2 = B))$$


### Model Exploration Detective Work

I've created a synthetic dataset for you to explore. The goal is to try and recover the true model. Note: that in practice we don't know a "true model."

```{r}
#| eval: false
#| echo: false
set.seed(09262025)
n <- 200
x1 <- seq(-3, 3, length.out = n)
x2 <- rnorm(n)
x3 <- rnorm(n)
x4 <- sample(c('A','B','C'), size = n, replace = T)

betaA <- -.5
betaB <- 0
betaC <- 1.5
beta1 <- .8
beta1_sq <- -.2
beta2 <- -.1
beta3 <- 0

pi <- invlogit(betaA * (x4=='A') + betaB * (x4 == 'B') + betaC * (x4 == 'C') + 
                 x1 * beta1 + x1^2 * beta1_sq + x2 * beta2 + x3 * beta3)

y <- rbinom(n, 10, pi)
write_csv(tibble(y =y, x1, x2, x3, x4), file = 'week7_data.csv')
```


I've given you a synthetic dataset with 200 binomial trials, where each binomial trial has 10 binary values.

```{r}
secret_data <- read_csv('https://raw.githubusercontent.com/STAT439/Data/refs/heads/main/week7_data.csv')
```


1. Plot the response (success out of 10 trials) against each of the potential covariates.

```{r}
secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x1)) +
  geom_smooth(method = 'loess', formula = 'y ~ x') +
  geom_jitter(alpha = .5, height = .03, width = .03) +
  theme_bw()
```

```{r}
secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x2)) +
  geom_smooth(method = 'loess', formula = 'y ~ x') +
  geom_jitter(alpha = .5, height = .03, width = .03) +
  theme_bw()
```

```{r}
secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x3)) +
  geom_smooth(method = 'loess', formula = 'y ~ x') +
  geom_jitter(alpha = .5, height = .03, width = .03) +
  theme_bw()
```

```{r}
secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x4, color = x4)) +
 # geom_smooth(method = 'loess', formula = 'y ~ x') +
  geom_violin() +
  geom_boxplot(width = .1) +
  geom_jitter(alpha = .75, height = .03, width = .3) +
  theme_bw() +
  theme(legend.position = 'none')
```

Discuss what you see.

2. Let's fit a start by fitting a simple logistic regression model `y ~ x1`

```{r}
ml <- glm(cbind(y, 10-y) ~ x1, data = secret_data, family = binomial)
summary(ml)
confint(ml)
```

Now add this model fit to your previous figure.

```{r}
fit_data <- tibble(x1 = secret_data$x1, y = secret_data$y, prop = invlogit(coef(ml)[[1]] + coef(ml)[[2]] * x1))
secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x1)) +
  geom_smooth(method = 'loess', formula = 'y ~ x') +
  geom_jitter(alpha = .5, height = .03, width = .03) +
  theme_bw() +
  geom_line(data = fit_data, color = 'red')
```

Let's also look directly at a type of residual (the standardized residual), which can be computed with `rstandard()`.

```{r}

tibble(x1 = secret_data$x1, resid = rstandard(ml)) |>
  ggplot(aes(y = resid, x = x1)) +
  geom_jitter(alpha = .5) +
  geom_smooth(method = 'loess', formula = 'y ~ x') +
  theme_bw() +
  geom_hline(yintercept = 0)

```

### Let's also consider x4

- Create an EDA that contains both x1 and x4, this should echo the model we will be fitting.

```{r}
secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x1, color = x4)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F) +
  geom_jitter(alpha = .5, height = .03, width = .03) +
  theme_bw()
```

- Add x4 to the model

```{r}
ml2 <- glm(cbind(y, 10-y) ~ x1 + x4 , data = secret_data, family = binomial)
summary(ml2)
confint(ml2)
```

- Plot fit

```{r, here}
fit_data <- tibble(x = rep(secret_data$x1,3), 
                   y = rep(secret_data$y,3), 
                   prop = c(invlogit( coef(ml2)[[1]] * secret_data$x1 + coef(ml2)[2]),
                            invlogit( coef(ml2)[[1]] * secret_data$x1 + coef(ml2)[3]),
                            invlogit( coef(ml2)[[1]] * secret_data$x1 + coef(ml2)[4])),
                   x4 = rep(c('A','B','C'), each = nrow(secret_data))) 


secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x1, color = x4)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F) +
  geom_jitter(alpha = .5, height = .03, width = .03) +
  theme_bw() +
  geom_line(data = fit_data |> rename(x1 = x),
            linetype = 2)
```



```{r}
tibble(x1 = secret_data$x1, resid = rstandard(ml2), x4 = secret_data$x4) |>
  ggplot(aes(y = resid, x = x1, color = x4)) +
  geom_jitter(alpha = .5) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F) +
  theme_bw() +
  geom_hline(yintercept = 0)

tibble(y= secret_data$y, resid = rstandard(ml2), x4 = secret_data$x4) |>
  ggplot(aes(y = resid, x = y, color = x4)) +
  geom_jitter(alpha = .5) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F) +
  theme_bw() +
  geom_hline(yintercept = 0)
```

### Which model is better?

One option is to use some sort of information criteria

- AIC = -2 $\times$ (log likelihood - # parameters) 
- we can do something similar, in spirit, with Bayesian approaches (Thursday Lab)

```{r}
AIC(ml)
AIC(ml2)
```

### Continue your model exploration and choose the best model you can find by AIC that also gives reasonable residuals

```{r}
ml3 <- glm(cbind(y, 10-y) ~  x1 + I(x1^2) + x4 - 1, data = secret_data, family = binomial)
summary(ml3)
confint(ml3)
AIC(ml3)

fit_data <- tibble(x = rep(secret_data$x1,3), 
                   y = rep(secret_data$y,3), 
                   prop = c(invlogit( coef(ml3)[[1]] * secret_data$x1 + coef(ml3)[[2]] * secret_data$x1^2 + coef(ml3)[3]),
                            invlogit( coef(ml3)[[1]] * secret_data$x1 + coef(ml3)[[2]] * secret_data$x1^2 + coef(ml3)[4]),
                            invlogit( coef(ml3)[[1]] * secret_data$x1 + coef(ml3)[[2]] * secret_data$x1^2 + coef(ml3)[5])),
                   x4 = rep(c('A','B','C'), each = nrow(secret_data))) 


secret_data |>
  mutate(prop = y / 10)|>
  ggplot(aes(y = prop, x = x1, color = x4)) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F) +
  geom_jitter(alpha = .5, height = .03, width = .03) +
  theme_bw() +
  geom_line(data = fit_data |> rename(x1 = x),
            linetype = 2)

tibble(x1 = secret_data$x1, resid = rstandard(ml3), x4 = secret_data$x4) |>
  ggplot(aes(y = resid, x = x1, color = x4)) +
  geom_jitter(alpha = .5) +
  geom_smooth(method = 'loess', formula = 'y ~ x', se = F) +
  theme_bw() +
  geom_hline(yintercept = 0)
```

```{r}
ml4 <- glm(cbind(y, 10-y) ~  x1 + x2 + I(x1^2) + x4 - 1, data = secret_data, family = binomial)
summary(ml4)
confint(ml4)
AIC(ml4)
```

```{r}
ml5 <- glm(cbind(y, 10-y) ~  x1 + x3 + I(x1^2) + x4 - 1, data = secret_data, family = binomial)
summary(ml4)
confint(ml4)
AIC(ml4)
```
